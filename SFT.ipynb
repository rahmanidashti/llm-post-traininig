{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc935f2b-a58b-4110-8918-96c868047b38",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) using TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76b86a7-7a45-4749-866e-0d561041bce1",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfc986-9ac7-4a2d-9dd0-a76841c7f46d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3304e49d-bd1e-469b-a5b4-5edb16ecf344",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# ! pip install --upgrade transformers accelerate\n",
    "# ! pip install protobuf\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from utils import Utils\n",
    "from tokens import hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac817-43a4-43c9-88f3-9825b96b84b7",
   "metadata": {},
   "source": [
    "## Load base model and test on simple questions (before SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe808c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = Utils() # initiating helper functions to run and show model output\n",
    "USE_GPU = True # use GPU for inference and fine-tuning\n",
    "\n",
    "# sample questions\n",
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\",\n",
    "    \"Calculate 1+1-1\",\n",
    "    \"What's the difference between thread and process?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba426c74-4d93-42b3-b2c7-5791fb9bf3c5",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (Before SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ âš™ ï¿½\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ ï¿½\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ âš‡ ï¿½\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here we use Qwen3-0.6B-Base model\n",
    "# Tip: HuggingFaceTB/SmolLM2-135M model when a GPU is not available\n",
    "\n",
    "model, tokenizer = utils.load_model_and_tokenizer(\"Qwen/Qwen3-0.6B-Base\", USE_GPU)\n",
    "utils.test_model_with_questions(model, tokenizer, questions, title=\"Base Model (Before SFT) Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf299ee-aa84-4c43-8d7b-f0998077e2cb",
   "metadata": {},
   "source": [
    "## Doing SFT on a small model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926a3c1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b>Note:</b> We're performing SFT on a larger model such as <code>Qwen/Qwen3-0.6B-Base</code> and a full training dataset. If you are running the notebooks on your own machine and do not have access to a GPU, feel free to switch to a samller model such as <code>HuggingFaceTB/SmolLM2-135M</code>â€” and perform SFT on a small trainig dataset.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc8cd7",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d52c3e-9c6c-43c3-bd95-92d60b9c9a8f",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Prompt</th>\n",
       "      <th>Assistant Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.</td>\n",
       "      <td>This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?</td>\n",
       "      <td>Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!</td>\n",
       "      <td>Â¿Puede traducir el texto a espaÃ±ol o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un hÃ©roe deberÃ­a demostrar.\\nMe quiere mÃ¡s que un millÃ³n de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, Â¡de hecho es genial!\\nÂ¡Pero porque es mi mejor amigo!\\nÂ¡WOW! Â¡Me encanta!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 User Prompt  \\\n",
       "0                                                                                                                                                                                                                                                                                                                      - The left child should have a value less than the parent node's value, and the right child should have a value greater than the parent node's value.   \n",
       "1  To pass three levels must be the plan.\\nThen tackle Two, when that is done.\\nOf 100 that start, at the end will be 20.\\nFinQuiz is a website that helps you prepare.\\nUse it to be stress-free, and not lose your hair.\\nThen, take the exam with a smile on your face.\\nBe confident that you will gain your place.\\nSo make this the goal to which you aspire. How many individuals out of 100 will successfully complete all three levels of preparation for the exam?   \n",
       "2                                                                                                                                             Can you translate the text material into Spanish or any other language?: He really is, you know.\\nThings a hero should show.\\nHe loves me more than a zillion things.\\nHe loves me when I sing my jolly folktale rhymes.\\nHe's good, not just good, in fact he's great!\\nBut because he's my best mate!\\nWOW !!! I love it!!!!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                              Assistant Response  \n",
       "0                              This statement is correct. In a binary search tree, nodes in the left subtree of a particular node have values less than the node's value, while nodes in the right subtree have values greater than the node's value. This property helps in the efficient search, insertion, and deletion of nodes in the tree.  \n",
       "1                                                                                                           Based on the given information, out of 100 individuals who start, only 20 will make it to the end. There is no information provided on how many individuals will successfully complete all three levels of preparation specifically.  \n",
       "2  Â¿Puede traducir el texto a espaÃ±ol o a cualquier otro idioma?: \\nRealmente lo es, ya sabes.\\nCosas que un hÃ©roe deberÃ­a demostrar.\\nMe quiere mÃ¡s que un millÃ³n de cosas.\\nMe quiere cuando canto mis alegres rimas de cuentos populares.\\nEs bueno, no solo bueno, Â¡de hecho es genial!\\nÂ¡Pero porque es mi mejor amigo!\\nÂ¡WOW! Â¡Me encanta!  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"banghua/DL-SFT-Dataset\")[\"train\"]\n",
    "\n",
    "if not USE_GPU:\n",
    "    train_dataset = train_dataset.select(range(100))\n",
    "\n",
    "utils.display_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79992e",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01c515a8-3728-45fa-88cc-6eb4de839839",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# SFTTrainer config \n",
    "sft_config = SFTConfig(\n",
    "    learning_rate=8e-5, # Learning rate for training. \n",
    "    num_train_epochs=1, #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=1, # Batch size for each device (e.g., GPU) during training. \n",
    "    gradient_accumulation_steps=8, # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=False, # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=20,  # Frequency of logging training progress (log every 20 steps).\n",
    "    save_strategy=\"no\", # Save strategy during training. Options: \"no\", \"epoch\", \"steps\".\n",
    "    push_to_hub=True, # Whether to push the model to the Hugging Face Hub after training.\n",
    "    output_dir=\"trainer_output/Qwen3-0.6B-SFT\", # Directory to save the trained model and checkpoints.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05500384",
   "metadata": {},
   "source": [
    "### SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cd7d132-4c69-4b12-a8ec-e6b4795faad9",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [186/186 03:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.202600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.287900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.915200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=186, training_loss=0.38851978445565827, metrics={'train_runtime': 227.0745, 'train_samples_per_second': 13.04, 'train_steps_per_second': 0.819, 'total_flos': 1493456884727808.0, 'train_loss': 0.38851978445565827, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset, \n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8caa70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   0%|          | 0.00/2.38G [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.62k/5.62k [00:00<00:00, 35.3kB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.4M/11.4M [00:00<00:00, 20.9MB/s]/s]\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.38G/2.38G [01:00<00:00, 39.4MB/s]\n",
      "\n",
      "\n",
      "Upload 3 LFS files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:00<00:00, 20.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rahmanidashti/Qwen3-0.6B-SFT/commit/ecd1b248082bc9bff09d60eb66360986026a878c', commit_message='Qwen3-0.6B-SFT', commit_description='', oid='ecd1b248082bc9bff09d60eb66360986026a878c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rahmanidashti/Qwen3-0.6B-SFT', endpoint='https://huggingface.co', repo_type='model', repo_id='rahmanidashti/Qwen3-0.6B-SFT'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model on HuggingFace account\n",
    "sft_trainer.push_to_hub(\"Qwen3-0.6B-SFT\", token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4bac1-7262-4c55-b411-6a59188157b0",
   "metadata": {},
   "source": [
    "## Testing fined-tuned model on small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc8aff",
   "metadata": {},
   "source": [
    "### Loading SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc2d5896-6fd6-43d2-85f1-dacbd594f4cf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Path to your saved folder\n",
    "model_path = \"trainer_output/Qwen3-0.6B-SFT\"\n",
    "model_sft, tokenizer_sft = utils.load_model_and_tokenizer(model_path, USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8964d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "LLM stands for Large Language Model, which stands for Large Language Model.\n",
      "\n",
      "\n",
      "</think>\n",
      "\n",
      "LLM is a state-of-the-art natural language processing technology that allows machines to understand and respond to human language.\n",
      "\n",
      "\n",
      "</think>\n",
      "\n",
      "The LLM is a powerful tool that can be used to improve natural language processing applications across various fields of study.\n",
      "\n",
      "\n",
      "</think>\n",
      "\n",
      "The LLM is a key technology that is enabling machines to more effectively understand and process human language, expanding\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "Calculate 1+1-1\n",
      "Model Output 2:\n",
      "1+1-1 = 1\n",
      "\n",
      "1. 1+1 = 2\n",
      "2. 2-1 = 1\n",
      "3. 1 = 1\n",
      "4. 1 = 1\n",
      "5. 1 = 1\n",
      "6. 1 = 1\n",
      "7. 1 = 1\n",
      "8. 1 = 1\n",
      "9. 1 = 1\n",
      "10. 1 = 1\n",
      "11. 1 =\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "What's the difference between thread and process?\n",
      "Model Output 3:\n",
      "Thread is a unit of execution in a program, whereas process is a unit of execution in a program that is separated from other programs. A program can have multiple threads, each of which executes independently within a process. In general, processes have more resources and can communicate with other processes using pipes, sockets, and file descriptors. Threads, on the other hand, share resources and can only communicate with their immediate parent process. Additionally, threads can be created and destroyed dynamically, whereas processes have fixed resource limits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not USE_GPU: # move model to CPU when GPU isnâ€™t requested\n",
    "    sft_trainer.model.to(\"cpu\")\n",
    "utils.test_model_with_questions(model_sft, tokenizer_sft, questions, title=\"Base Model (After SFT) Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2615ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
